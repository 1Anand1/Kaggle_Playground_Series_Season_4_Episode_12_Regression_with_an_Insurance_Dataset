{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importing other required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing required ML libraries\n",
    "from sklearn.model_selection import train_test_split,KFold,GridSearchCV\n",
    "\n",
    "# Importing the required regressors\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor,XGBClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Importing the score maker\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Checking all the columns \n",
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepaths for train and test dataset\n",
    "train_dataset=r'C:\\Users\\DELL\\playground-series-s4e12\\train.csv'\n",
    "test_dataset=r'C:\\Users\\DELL\\playground-series-s4e12\\test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the required training & testing data into a pandas dataframe\n",
    "df_train=pd.read_csv(train_dataset)\n",
    "df_test=pd.read_csv(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the basic data available\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the null values\n",
    "(100*df_train.isnull().sum()/len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the first five rows\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the Month & Year, then dropping the Policy Start Date column\n",
    "def date_time_handler(df:pd.DataFrame):\n",
    "  df['Policy Start Date']=pd.to_datetime(df['Policy Start Date'])\n",
    "  df['Month']=df['Policy Start Date'].dt.month.astype('object') # Making Month object data type considering its usecase\n",
    "  df['Year']=df['Policy Start Date'].dt.year.astype('object') # Making Year object data type considering its usecase\n",
    "  df.drop(['Policy Start Date'],axis=1,inplace=True)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the date time handler function on train and test dataframe\n",
    "df_train=date_time_handler(df_train)\n",
    "df_test=date_time_handler(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a function which will extract the columns which have missing values below certain threshold\n",
    "def missing_value_col_below_threshold(df:pd.DataFrame,threshold_value:int)->list:\n",
    "  \n",
    "  below_threshold_missing_value=[]\n",
    "\n",
    "  if not isinstance(threshold_value,int):\n",
    "    raise TypeError(f\"The given value for threshold is {threshold_value}, which is not an integer\")\n",
    "  \n",
    "  if not isinstance(df,pd.DataFrame):\n",
    "    raise TypeError(f'The given value of the dataframe is not a pandas dataframe')\n",
    "  \n",
    "  rqd_result=(100*df.isnull().sum()/len(df))\n",
    "\n",
    "  for each_col,each_val in rqd_result.items():\n",
    "    if each_val<=threshold_value and each_val!=0:\n",
    "      below_threshold_missing_value.append((each_col,each_val,df[each_col].dtype))\n",
    "\n",
    "  below_threshold_missing_value=sorted(below_threshold_missing_value,key=lambda x:x[1],reverse=True)\n",
    "\n",
    "  return below_threshold_missing_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if train & test data set has similar columns\n",
    "\n",
    "missing_train=np.array([each_col for each_col,_,_ in missing_value_col_below_threshold(df_train,50)])\n",
    "missing_test=np.array([each_col for each_col,_,_ in missing_value_col_below_threshold(df_test,50)])\n",
    "\n",
    "print(missing_train==missing_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also checking if the missing values in these columns are close to each other\n",
    "\n",
    "missing_train_values=np.array([each_val for _,each_val,_ in missing_value_col_below_threshold(df_train,50)])\n",
    "missing_test_values=np.array([each_val for _,each_val,_ in missing_value_col_below_threshold(df_test,50)])\n",
    "\n",
    "print(missing_train_values-missing_test_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the columns in train data set which have missing values below or equal to 15 %\n",
    "missing_value_col_below_threshold(df_train,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a function to extract the values from the missing_value_col_below_threshold function\n",
    "# Replacing the missing values in respective columns based upon their data types\n",
    "\n",
    "def replacing_missing_values(df:pd.DataFrame,missing_value_tuple:list[tuple]):\n",
    "  result=missing_value_tuple\n",
    "  col_name,_,data_types =zip(*result)\n",
    "\n",
    "  for col,data_type in zip(col_name,data_types):\n",
    "    if data_type=='float64':\n",
    "      df[col]=df[col].fillna(df[col].median())\n",
    "    elif data_type=='object':\n",
    "      df[col]=df[col].fillna(df[col].mode()[0])\n",
    "  \n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the missing values in the train dataframe by the median or mode if the missing value percentage is less than or equal to 15\n",
    "col_to_impute=missing_value_col_below_threshold(df_train,15)\n",
    "df_train=replacing_missing_values(df_train,col_to_impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the missing value columns\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the values in the Previous Claims & Occupation columns\n",
    "print(df_train['Previous Claims'].unique())\n",
    "print(df_train.Occupation.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning the columns which are having higher percentage of missing values\n",
    "col_with_high_missing_values=missing_value_col_below_threshold(df_train,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert the Occupation column into a numerical variable so that XGBoost classifier will be able to impute missing values\n",
    "def occupation_converter(occu:str)->int:\n",
    "  if occu==\"Self-Employed\":\n",
    "    return 0\n",
    "  elif occu==\"Employed\":\n",
    "    return 1\n",
    "  elif occu==\"Unemployed\":\n",
    "    return 2\n",
    "  elif pd.isnull(occu): # np.Nan is special type of float so need to remove it \n",
    "    return -1 # To mark missing values\n",
    "  else:\n",
    "    return -1 # to handle unexpected values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formulating a function to impute the missing values for columns with high percentage of missing values\n",
    "def preprocessing_high_missing_value_col(df:pd.DataFrame,list_of_tuples:list[tuple])->pd.DataFrame:\n",
    "  result=list_of_tuples # Taking the list of tuples which contain the columns with high missing values\n",
    "  high_missing_col,_,_=zip(*result) # Extracting the columns with high missing values\n",
    "  \n",
    "  for each_col in high_missing_col:\n",
    "    if each_col==\"Previous Claims\":\n",
    "      df[each_col]=df[each_col].fillna(-1).astype(\"int64\") # Filling missing values with a placeholder of -1 & changing the datatype for XGBClassifer application\n",
    "    elif each_col==\"Occupation\":\n",
    "      df[each_col]=df[each_col].apply(lambda x:occupation_converter(x))\n",
    "    else:\n",
    "      pass\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the train dataset ready for the final missing value imputation\n",
    "df_train=preprocessing_high_missing_value_col(df_train,col_with_high_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for the missing value imputation using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a helper function for the conversion of the object columns in to the category columns\n",
    "\n",
    "def object_to_category(df:pd.DataFrame)->pd.DataFrame:\n",
    "  return df.astype({col:'category' for col in df.select_dtypes(include='object').columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the suitable datasets\n",
    "df_prev_claim=df_train.drop(columns=['Occupation','Premium Amount','id'],axis=1)\n",
    "df_occu=df_train.drop(columns=['Previous Claims','Premium Amount','id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the first five rows\n",
    "df_occu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the train and test data from occupation columns\n",
    "df_occu_train=df_occu[df_occu['Occupation']!=-1]\n",
    "df_occu_test=df_occu[df_occu['Occupation']==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the label\n",
    "y_occu=df_occu_train.pop('Occupation')\n",
    "X_occu=df_occu_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost needs all columns to be of numeric type but to handle this with some advanced functionalities, I needed to convert the object columns into categorical columns\n",
    "X_occu=object_to_category(X_occu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using XGB Classifier\n",
    "xgb_occu=XGBClassifier(objective='multi:softmax',\n",
    "                       num_class=y_occu.nunique(),\n",
    "                       eval_metric='merror',\n",
    "                       max_depth=5,\n",
    "                       learning_rate=0.01,\n",
    "                       n_estimators=100,\n",
    "                       tree_method='hist',\n",
    "                       enable_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the xgb classifier object on the y_occu & X_occu\n",
    "xgb_occu.fit(X_occu,y_occu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the occupation column from the df_occu_test\n",
    "df_occu_test.drop(['Occupation'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before predicitng on the df_occu_test, I need to make sure that the object datatype columns are converted into category datatype\n",
    "df_occu_test=object_to_category(df_occu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the occupation column from the df_occu_test\n",
    "y_occu_test=xgb_occu.predict(df_occu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now adding the y_occu_test directly into the df_train\n",
    "df_train.loc[df_train['Occupation']==-1,'Occupation']=y_occu_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the first 5 rows of df_prev_claim\n",
    "df_prev_claim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the train and the test from previous claims column\n",
    "df_prev_claim_train=df_prev_claim[df_prev_claim['Previous Claims']!=-1]\n",
    "df_prev_claim_test=df_prev_claim[df_prev_claim['Previous Claims']==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the previous claims labels\n",
    "y_prev_claim=df_prev_claim_train.pop('Previous Claims')\n",
    "X_prev_claim=df_prev_claim_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost needs all columns to be of numeric type but to handle this with some advanced functionalities, I needed to convert the object columns into categorical columns\n",
    "X_prev_claim=object_to_category(X_prev_claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using XGB Classifier\n",
    "xgb_prev_claim=XGBClassifier(objective='multi:softmax',\n",
    "                             num_class=y_prev_claim.nunique(),\n",
    "                             eval_metric='merror',\n",
    "                             max_depth=5,\n",
    "                             learning_rate=0.01,\n",
    "                             n_estimators=100,\n",
    "                             tree_method='hist',\n",
    "                             enable_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting XGB Classifier object on X_prev_claim & y_prev_claim\n",
    "xgb_prev_claim.fit(X_prev_claim,y_prev_claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the unnecessary column of previous claims from the df_prev_claim_test\n",
    "df_prev_claim_test.drop(['Previous Claims'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the datatype of the object columns to the category for using the XGB Classifier \n",
    "df_prev_claim_test=object_to_category(df_prev_claim_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the y_prev_claims_test\n",
    "y_prev_claims_test=xgb_prev_claim.predict(df_prev_claim_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now adding the y_prev_claims_test to df_train\n",
    "df_train.loc[df_train['Previous Claims']==-1,'Previous Claims']=y_prev_claims_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the null values \n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the unique elements in previous claims and occupation\n",
    "print(df_train['Occupation'].unique())\n",
    "print(df_train['Previous Claims'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that I have got all the values in the Occupation column after missing value imputation, I would rever the effect of occupation_converter \n",
    "occupation_mapping={0:\"Self-Employed\",1:\"Employed\",2:\"Unemployed\"}\n",
    "\n",
    "df_train['Occupation']=df_train['Occupation'].apply(lambda x : occupation_mapping.get(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting EDA as the missing value imputation has been completed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the info of the df_train\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the id column from the df_train\n",
    "df_train.drop(columns=['id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the numeric columns for plotting the box plots\n",
    "num_col=set(df_train.select_dtypes(include=['float64','int64']).columns)\n",
    "high_unique_values=set([col for col,unique_value in df_train.nunique().items() if unique_value>=10])\n",
    "rqd_num_col=num_col.intersection(high_unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplot of numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the box plots\n",
    "n_cols=2 # I always want to see only two columns\n",
    "n_rows=((len(rqd_num_col)+1)//n_cols)\n",
    "fig,axes=plt.subplots(nrows=n_rows,ncols=n_cols,figsize=(20,20))\n",
    "axes=axes.flatten()\n",
    "\n",
    "# Plotting the box plots\n",
    "for col_no,each_col in enumerate(rqd_num_col):\n",
    "  sns.boxplot(data=df_train,x=each_col,ax=axes[col_no])\n",
    "  axes[col_no].set_title(f'Box-plot for {each_col}')\n",
    "\n",
    "# Deleting unnecessary axes\n",
    "for del_plot in range(col_no+1,len(axes)):\n",
    "  fig.delaxes(ax=axes[del_plot])\n",
    "\n",
    "# Adjusting the visualization of the graphs\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the histograms of the numeric features to understand their distribution\n",
    "n_cols=2\n",
    "n_rows=((len(rqd_num_col)+1)//n_cols)\n",
    "fig,axes=plt.subplots(nrows=n_rows,ncols=n_cols,figsize=(20,20))\n",
    "axes=axes.flatten()\n",
    "\n",
    "# Plotting the histograms for the numeric columns\n",
    "for col_no,each_col in enumerate(rqd_num_col):\n",
    "  ax=axes[col_no]\n",
    "  ax.hist(df_train[each_col])\n",
    "  ax.set_title(f'Histogram for {each_col}')\n",
    "  ax.set_xlabel(f'{each_col}')\n",
    "  ax.set_ylabel('Frequency')\n",
    "\n",
    "# Deleting the unnncessary axes\n",
    "for axes_to_del in range(col_no+1,len(axes)):\n",
    "  fig.delaxes(ax=axes[axes_to_del])\n",
    "\n",
    "# Making the adjustments for plotting the histograms effectively\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the columns which are object\n",
    "object_columns=list(df_train.select_dtypes(include='object').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the countplots of the object columns \n",
    "n_cols=2\n",
    "n_rows=((len(object_columns)+1)//n_cols)\n",
    "fig,axes=plt.subplots(nrows=n_rows,ncols=n_cols,figsize=(20,20))\n",
    "axes=axes.flatten()\n",
    "\n",
    "# plotting the countplots\n",
    "for col_no,each_col in enumerate(object_columns):\n",
    "  sns.countplot(data=df_train,x=each_col,ax=axes[col_no])\n",
    "  axes[col_no].set_title(f'Count plot for {each_col}')\n",
    "\n",
    "# Deleting unnecessary axes\n",
    "for axes_to_del in range(col_no+1,len(axes)):\n",
    "  fig.delaxes(ax=axes[axes_to_del])\n",
    "\n",
    "# Arranging the layout as per the need\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the basic info about df_train once again\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a dummy dataframe from the object columns\n",
    "df_dummies=pd.get_dummies(data=df_train,columns=object_columns,drop_first=True)\n",
    "\n",
    "# Changing the boolean columns into 1/0 for correlation matrix creation\n",
    "bool_col=df_dummies.select_dtypes(include='bool').columns\n",
    "df_dummies[bool_col]=df_dummies[bool_col].astype(int)\n",
    "\n",
    "# Making the premium amount as the first column for easier visualization & inference creation\n",
    "dummy_columns=['Premium Amount']+[col for col in df_dummies.columns if col!='Premium Amount']\n",
    "df_dummies=df_dummies[dummy_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a correlation matrix\n",
    "corr_mat=df_dummies.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heat map\n",
    "plt.figure(figsize=(30,30))\n",
    "sns.heatmap(data=corr_mat,cmap='coolwarm',annot=True,fmt='.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I decided to check the basic info of the all columns in the df_dummies --> I decided to drop the dummy columns i.e.columns with datatype = int 32\n",
    "df_dummies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the columns from the df_dummies which are not int32\n",
    "col_int_32=df_dummies.select_dtypes(include='int32')\n",
    "col_not_int_32=[col for col in df_dummies.columns if col not in col_int_32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a correlation matrix of the columns which are not obtained by making dummies\n",
    "corr_mat_reduced=df_dummies[col_not_int_32].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the simplified heatmap \n",
    "plt.figure(figsize=(10,5))\n",
    "sns.heatmap(data=corr_mat_reduced,cmap='coolwarm',annot=True,fmt='.3f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the training phase of the ML algorithm - By Using K Fold CV & Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting extracting the dependent & independent variables\n",
    "X_train=df_train[[col for col in df_train.columns if col!='Premium Amount']]\n",
    "y_train=df_train['Premium Amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chaniging the object columns to category type for using them in Cat Boost Algorithm\n",
    "X_train=object_to_category(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a manual function to calculate Root mean squared logarithmic error ( RMSLE )\n",
    "\n",
    "def rmsle(y_actual,y_predicted):\n",
    "\n",
    "  \"\"\"\n",
    "  This function is made to calculate the RMSLE ( Root Mean Squared Logarithmic Error)\n",
    "  \"\"\"\n",
    "  return np.sqrt(np.mean((np.log1p(y_actual)-np.log1p(y_predicted))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a custome scorer for calculating the RMSLE\n",
    "rmsle_scorer=make_scorer(rmsle,greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up K Folde cross validation\n",
    "kf=KFold(n_splits=3,shuffle=True,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyperparameter grids for all regression algorithms\n",
    "\n",
    "rf_params_grid={\n",
    "  'n_estimators':[50,100],\n",
    "  'max_depth':[3,5],\n",
    "  'min_samples_split':[3,5]\n",
    "}\n",
    "\n",
    "\n",
    "xgb_params_grid={\n",
    "  'n_estimators':[100,200],\n",
    "  'max_depth':[3,5],\n",
    "  'learning_rate':[0.01,0.1]\n",
    "}\n",
    "\n",
    "\n",
    "cat_params_grid={\n",
    "  'iterations':[50,75],\n",
    "  'depth':[2,5],\n",
    "  'learning_rate':[0.05,0.1]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a list of the categorical features\n",
    "cat_features_list=[col for col in X_train.select_dtypes(include='category').columns ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the numerical columns\n",
    "num_features_list=[col for col in X_train.columns if col not in cat_features_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a dataframe fo the dummies from the categorical columns\n",
    "X_train_dummies=pd.get_dummies(data=X_train,columns=cat_features_list,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now checking the columns from the X_train_dummies which are introduced due to one hot encoding & chaning their datatype from boolean to integer\n",
    "final_dummy_columns=[col for col in X_train_dummies.columns if col not in num_features_list]\n",
    "X_train_dummies[final_dummy_columns]=X_train_dummies[final_dummy_columns].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting the model training by using the Random Forest Regressor\n",
    "rf_model=RandomForestRegressor(random_state=42)\n",
    "rf_model_grid_search=GridSearchCV(estimator=rf_model,param_grid=rf_params_grid,cv=kf,scoring=rmsle_scorer,verbose=0)\n",
    "rf_model_grid_search.fit(X=X_train_dummies,y=y_train)\n",
    "\n",
    "# Checking the params which provide the best score and also checking the best scores for the random forest\n",
    "print(f'The params which provide the best score for the Random Forest are {rf_model_grid_search.best_params_}')\n",
    "print(f'The best scores obtained are {rf_model_grid_search.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting the model training by using the XGBoost Regressor\n",
    "xgb_model=XGBRegressor(random_state=42,tree_method='hist',enable_categorical=True)\n",
    "xgb_model_grid_search=GridSearchCV(estimator=xgb_model,param_grid=xgb_params_grid,cv=kf,scoring=rmsle_scorer,verbose=0)\n",
    "xgb_model_grid_search.fit(X=X_train,y=y_train)\n",
    "\n",
    "# Checking the params which provide the best score & also checking the best scores\n",
    "print(f' The params which provide the best score for the XGBoost are {xgb_model_grid_search.best_params_}')\n",
    "print(f' The best scores obtained are {xgb_model_grid_search.best_score_}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting by using CAT Boost Regressor --> As CATBoost does not need any prior preprocessing for categorical features\n",
    "cat_model=CatBoostRegressor(random_state=42,cat_features=cat_features_list)\n",
    "cat_model_grid_search=GridSearchCV(estimator=cat_model,param_grid=cat_params_grid,cv=kf,scoring=rmsle_scorer,verbose=0)\n",
    "cat_model_grid_search.fit(X=X_train,y=y_train)\n",
    "\n",
    "# Checking the params which provide the best scores & also checking the best scores\n",
    "print(f'The best params for CatBoost are: {cat_model_grid_search.best_params_}')\n",
    "print(f'The best score for CatBoost are: {cat_model_grid_search.best_score_}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
